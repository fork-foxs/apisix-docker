#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

version: "3.8"

configs:
  apisix_config:
    file: ./apisix_conf/config.yaml
  dashboard_config:
    file: ./dashboard_conf/config.yaml
  prometheus_config:
    file: ./prometheus_conf/prometheus.yml
  grafana_config:
    file: ./grafana_conf/config/grafana.ini
  grafana_datasources:
    file: ./grafana_conf/provisioning/datasources/all.yaml
  grafana_dashboards:
    file: ./grafana_conf/provisioning/dashboards/all.yaml
  grafana_dashboard_json:
    file: ./grafana_conf/dashboards/apisix-grafana-dashboard.json
  web1_config:
    file: ./upstream/web1.conf
  web2_config:
    file: ./upstream/web2.conf

services:
  apisix:
    image: apache/apisix:${APISIX_IMAGE_TAG:-3.14.1-debian}
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 8G
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    configs:
      - source: apisix_config
        target: /usr/local/apisix/conf/config.yaml
        mode: 0444
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/v1/schema"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    environment:
      - APISIX_STAND_ALONE=false
      - APISIX_NODE_LISTEN=9080
      - APISIX_ADMIN_LISTEN=9180
      - APISIX_INSTANCE_NAME={{.Task.Slot}}
    ports:
      - "9180:9180/tcp"
      - "9080:9080/tcp"
      - "9091:9091/tcp"
      - "9443:9443/tcp"
      - "9092:9092/tcp"
      - "9100:9100/tcp"
    networks:
      - apisix

  etcd:
    image: bitnamilegacy/etcd:3.5.11
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    volumes:
      - etcd_data:/bitnami/etcd
    environment:
      ETCD_ENABLE_V2: "true"
      ALLOW_NONE_AUTHENTICATION: "yes"
      ETCD_ADVERTISE_CLIENT_URLS: "http://etcd:2379"
      ETCD_LISTEN_CLIENT_URLS: "http://0.0.0.0:2379"
      ETCD_INITIAL_ADVERTISE_PEER_URLS: "http://etcd:2380"
      ETCD_LISTEN_PEER_URLS: "http://0.0.0.0:2380"
      ETCD_INITIAL_CLUSTER: "etcd=http://etcd:2380"
      ETCD_HEARTBEAT_INTERVAL: "250"
      ETCD_ELECTION_TIMEOUT: "5000"
      ETCD_MAX_SNAPSHOTS: "5"
      ETCD_MAX_WALS: "5"
      ETCD_QUOTA_BACKEND_BYTES: "4294967296"
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    ports:
      - "2379:2379/tcp"
      - "2380:2380/tcp"
    networks:
      - apisix

  web1:
    image: nginx:1.19.0-alpine
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    configs:
      - source: web1_config
        target: /etc/nginx/nginx.conf
        mode: 0444
    ports:
      - "9081:80/tcp"
      - "9083:8080/tcp"
    environment:
      - NGINX_PORT=80
      - NGINX_HOST=web1
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - apisix

  web2:
    image: nginx:1.19.0-alpine
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    configs:
      - source: web2_config
        target: /etc/nginx/nginx.conf
        mode: 0444
    ports:
      - "9082:80/tcp"
      - "9084:8080/tcp"
    environment:
      - NGINX_PORT=80
      - NGINX_HOST=web2
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - apisix

  prometheus:
    image: prom/prometheus:v2.25.0
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    configs:
      - source: prometheus_config
        target: /etc/prometheus/prometheus.yml
        mode: 0444
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    ports:
      - "9090:9090"
    networks:
      - apisix

  grafana:
    image: grafana/grafana:7.3.7
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana_data:/var/lib/grafana
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    ports:
      - "3002:3000"
    configs:
      - source: grafana_config
        target: /etc/grafana/grafana.ini
        mode: 0444
      - source: grafana_datasources
        target: /etc/grafana/provisioning/datasources/all.yaml
        mode: 0444
      - source: grafana_dashboards
        target: /etc/grafana/provisioning/dashboards/all.yaml
        mode: 0444
      - source: grafana_dashboard_json
        target: /var/lib/grafana/dashboards/apisix-grafana-dashboard.json
        mode: 0444
    networks:
      - apisix

  dashboard:
    image: harbor.fintechsys.net/api-gateway-six/apisix-dashboard:latest
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '2'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 8G
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    configs:
      - source: dashboard_config
        target: /usr/local/apisix-dashboard/conf/conf.yaml
        mode: 0444
    environment:
      - CONF_PATH=/usr/local/apisix-dashboard/conf/conf.yaml
      - ETCD_ENDPOINTS=http://etcd:2379
      - ETCD_PREFIX=/apisix
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    ports:
      - "9000:9000/tcp"
    networks:
      - apisix

networks:
  apisix:
    driver: overlay
    attachable: true
    labels:
      - "com.docker.stack.namespace=apisix-stack"

volumes:
  etcd_data:
    driver: local
    labels:
      - "com.docker.stack.namespace=apisix-stack"
  prometheus_data:
    driver: local
    labels:
      - "com.docker.stack.namespace=apisix-stack"
  grafana_data:
    driver: local
    labels:
      - "com.docker.stack.namespace=apisix-stack"